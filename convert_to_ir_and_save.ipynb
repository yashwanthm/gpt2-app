{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Save the Model\n",
    "\n",
    "To save this model so that you can use it from various locations, including other notebooks or the model server, upload it to s3-compatible storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the required packages and define a function for the upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "git_repo = \"https://huggingface.co/gpt2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt2'...\n",
      "remote: Enumerating objects: 87, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 87 (delta 17), reused 13 (delta 13), pack-reused 66 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (87/87), 1.65 MiB | 7.49 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone $git_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_name = os.path.basename(git_repo)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openvino-dev in /opt/app-root/lib/python3.9/site-packages (2024.2.0)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.42.4)\n",
      "Requirement already satisfied: torch in /opt/app-root/lib/python3.9/site-packages (2.3.1)\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.16.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: defusedxml>=0.7.1 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (0.7.1)\n",
      "Requirement already satisfied: requests>=2.25.1 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (2.32.2)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (6.0.1)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (2024.1.0)\n",
      "Requirement already satisfied: openvino==2024.2.0 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (2024.2.0)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (24.0)\n",
      "Requirement already satisfied: networkx<=3.1.0 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (3.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in /opt/app-root/lib/python3.9/site-packages (from openvino-dev) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/app-root/lib/python3.9/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/app-root/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/app-root/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/app-root/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/app-root/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /opt/app-root/lib/python3.9/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/app-root/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/app-root/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/app-root/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/app-root/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Collecting protobuf>=3.20.2\n",
      "  Downloading protobuf-5.27.2-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.3/309.3 kB\u001b[0m \u001b[31m184.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.25.1->openvino-dev) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.25.1->openvino-dev) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.25.1->openvino-dev) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.25.1->openvino-dev) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.16.1 protobuf-5.27.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openvino-dev transformers torch onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Onnx version of GPT\n",
    "Download the model from https://huggingface.co/openai-community/gpt2 and convert it into onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save the model in the ONNX format\n",
    "import torch\n",
    "\n",
    "# Create the 'onnx' directory if it doesn't exist\n",
    "os.makedirs(\"gpt2-onnx\", exist_ok=True)\n",
    "\n",
    "# Save the model in the ONNX format\n",
    "dummy_input = torch.randint(100, (1, 10))  # dummy input for tracing\n",
    "torch.onnx.export(model, dummy_input, \"gpt2-onnx/gpt2.onnx\", input_names=[\"input\"], output_names=[\"output\"], dynamic_axes={\"input\": [0, 1], \"output\": [0, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to IR Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: [ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /opt/app-root/src/gpt2_ir/gpt2.xml\n",
      "[ SUCCESS ] BIN file: /opt/app-root/src/gpt2_ir/gpt2.bin\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the Model Optimizer\n",
    "mo_command = [\n",
    "    \"mo\",\n",
    "    \"--input_model\", \"gpt2-onnx/gpt2.onnx\",\n",
    "    \"--input_shape\", \"[1,10]\",\n",
    "    \"--output_dir\", \"gpt2_ir\"\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "result = subprocess.run(mo_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print the output and error (if any)\n",
    "print(\"stdout:\", result.stdout)\n",
    "print(\"stderr:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.34.145-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore\n",
      "  Downloading botocore-1.34.145-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.9/site-packages (from botocore) (2.9.0.post0)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed boto3-1.34.145 botocore-1.34.145 jmespath-1.0.1 s3transfer-0.10.2 urllib3-1.26.19\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "#upload the model directory without git\n",
    "def upload_directory_to_s3(local_directory, s3_prefix, remove_safetensors=True):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            if \".git\" in relative_path:\n",
    "                print(f\"skipping {relative_path}\")\n",
    "                continue\n",
    "            # if remove_safetensors and \".safetensors\" in relative_path:\n",
    "            #     print(f\"skipping {relative_path}\")\n",
    "            #     continue\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check the Storage Bucket\n",
    "\n",
    "In your S3 bucket, under the `models` upload prefix, run the `list_object` command. As best practice, to avoid mixing up model files, keep only one model and its required files in a given prefix or directory. This practice allows you to download and serve a directory with all the files that a model requires. \n",
    "\n",
    "If this is the first time running the code, this cell will have no output or the fraud model from the predictive AI/ML exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload and check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to upload the `models` folder in a rescursive fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_ir/gpt2.xml -> models/gpt2_ir/gpt2.xml\n",
      "gpt2_ir/gpt2.bin -> models/gpt2_ir/gpt2.bin\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2_ir\"\n",
    "upload_directory_to_s3(model_name, f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-onnx/gpt2.onnx -> models/gpt2-onnx/gpt2.onnx\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-onnx\"\n",
    "upload_directory_to_s3(model_name, f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To confirm this worked, run the `list_objects` function again:\n",
    "\n",
    "This time, you should see files listed in the directory/prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gpt2_ir/gpt2.bin\n",
      "models/gpt2_ir/gpt2.xml\n"
     ]
    }
   ],
   "source": [
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "Now that you've saved the model to s3 storage, you can refer to the model by using the same data connection to serve the model as an API.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
